{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model definition: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in the very beginning we import the related library we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import AveragePooling2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "import densenet\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step:\n",
    "### we shoud design conv_factory adding bottleneck layer(which means  bn-1X1convolution-3X3convolution).\n",
    "### a bottle layer is a layer to increase the effiency by decreasing number of features .\n",
    "### here we use 1x1 convolution to decrease features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-3-ca2eb2c950fd>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-ca2eb2c950fd>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    x =\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def conv_factory(x, concat_axis, nb_filter,\n",
    "                 dropout_rate=None, weight_decay=1E-4):\n",
    "    \"\"\"Apply BatchNorm, Relu 3x3Conv2D, optional dropout\n",
    "    :param x: Input keras network\n",
    "    :param concat_axis: int -- index of contatenate axis\n",
    "    :param nb_filter: int -- number of filters\n",
    "    :param dropout_rate: int -- dropout rate\n",
    "    :param weight_decay: int -- weight decay factor\n",
    "    :returns: keras network with b_norm, relu and Conv2D added\n",
    "    :rtype: keras network\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\" -------------------------------------tips:\n",
    "                        first layer:bn with gamma_regularizer=l2,with beta_regularizer=l2(the decay rate use the pass-in params)\n",
    "                        second layer:  use relu as activation\n",
    "                        third layer: add conv2D layer,filternum use the conv_factory params,size=3X3,padding=same,\n",
    "                        not use bias,kernel_initializer=he_uniform,kernel_regularizer=l2(with pass-in decay rate)\n",
    "\"\"\"\n",
    "#code starts\n",
    "\n",
    "    x =\n",
    "    x = \n",
    "    x = \n",
    "#code ends\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second step:\n",
    "### then we define the layer between denseblocks---------transition layer.\n",
    "### transition layer is the layer intended to decrease filters number(by setting the theta params )there is not a compression here so theta is not an issue you should concerned about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-8c2e9397996f>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-8c2e9397996f>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    x =\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def transition(x, concat_axis, nb_filter,\n",
    "               dropout_rate=None, weight_decay=1E-4):\n",
    "    \"\"\"Apply BatchNorm, Relu 1x1Conv2D, optional dropout and Maxpooling2D\n",
    "    :param x: keras model\n",
    "    :param concat_axis: int -- index of contatenate axis\n",
    "    :param nb_filter: int -- number of filters\n",
    "    :param dropout_rate: int -- dropout rate\n",
    "    :param weight_decay: int -- weight decay factor\n",
    "    :returns: model\n",
    "    :rtype: keras model, after applying batch_norm, relu-conv, dropout, maxpool\n",
    "    \"\"\"\n",
    "''' -------------------------------------tips:\n",
    "                        first layer:bn with gamma_regularizer=l2,with beta_regularizer=l2(the decay rate use the pass-in params)\n",
    "                        second layer:use relu as activation\n",
    "                        third layer: add conv2D layer,filternum use the conv_factory params,size=1X1,padding=same,\n",
    "                        not use bias,kernel_initializer=he_uniform,kernel_regularizer=l2(with pass-in decay rate)\n",
    "                        pooling :use average poolingsize=2,2,stride=2,2\n",
    "                        \n",
    "'''\n",
    "# code starts\n",
    "    x = \n",
    "    x = \n",
    "    x =\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = \n",
    "# code ends\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## third step:\n",
    "### we shoud design a structure denoting a dense block by adding several convolution factory layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-85984041f53b>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-85984041f53b>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    for i in range(nb_layers):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def denseblock(x, concat_axis, nb_layers, nb_filter, growth_rate,\n",
    "               dropout_rate=None, weight_decay=1E-4):\n",
    "    \"\"\"Build a denseblock where the output of each\n",
    "       conv_factory is fed to subsequent ones\n",
    "    :param x: keras model\n",
    "    :param concat_axis: int -- index of contatenate axis\n",
    "    :param nb_layers: int -- the number of layers of conv_\n",
    "                      factory to append to the model.\n",
    "    :param nb_filter: int -- number of filters\n",
    "    :param dropout_rate: int -- dropout rate\n",
    "    :param weight_decay: int -- weight decay factor\n",
    "    :returns: keras model with nb_layers of conv_factory appended\n",
    "    :rtype: keras model\n",
    "    \"\"\"\n",
    "\n",
    "    list_feat = [x]\n",
    "\n",
    "''' -------------------------------------tips:\n",
    "                        first layer:adding a conv_factory layer(with pass-in params)\n",
    "                        \n",
    "'''\n",
    "    for i in range(nb_layers):\n",
    "#code starts\n",
    "        x =\n",
    "#code ends\n",
    "        list_feat.append(x)\n",
    "        x = Concatenate(axis=concat_axis)(list_feat)\n",
    "        nb_filter += growth_rate\n",
    "\n",
    "    return x, nb_filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fourth step:\n",
    "### finally we combine the modules that we defined to create our own DenseNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNet(nb_classes, img_dim, depth, nb_dense_block, growth_rate,\n",
    "             nb_filter, dropout_rate=None, weight_decay=1E-4):\n",
    "    \"\"\" Build the DenseNet model\n",
    "    :param nb_classes: int -- number of classes\n",
    "    :param img_dim: tuple -- (channels, rows, columns)\n",
    "    :param depth: int -- how many layers\n",
    "    :param nb_dense_block: int -- number of dense blocks to add to end\n",
    "    :param growth_rate: int -- number of filters to add\n",
    "    :param nb_filter: int -- number of filters\n",
    "    :param dropout_rate: float -- dropout rate\n",
    "    :param weight_decay: float -- weight decay\n",
    "    :returns: keras model with nb_layers of conv_factory appended\n",
    "    :rtype: keras model\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        concat_axis = 1\n",
    "    elif K.image_dim_ordering() == \"tf\":\n",
    "        concat_axis = -1\n",
    "\n",
    "    model_input = Input(shape=img_dim)\n",
    "\n",
    "    assert (depth - 4) % 3 == 0, \"Depth must be 3 N + 4\"\n",
    "\n",
    "    # layers in each dense block\n",
    "    nb_layers = int((depth - 4) / 3)\n",
    "\n",
    "''' -------------------------------------tips:\n",
    "                        first conv layer:only a conv2d,kernel_initializer=he_uniform,padding=same,not using bias,with l2 kernel_kernel_regularizer\n",
    "                        other params use pass-in params.\n",
    "                        dense block:adding blocks using pass-in params. \n",
    "                        transition layer:adding transition layer using pass-in params.\n",
    "                        last denseblock: adding a bn layer with gamma_regularizer=l2,with beta_regularizer=l2(the decay rate use the pass-in params)\n",
    "                        adding a relu activation,then use global averagepooling,then connecting to a full-connected layer\n",
    "                        with softmax-activation,l2-kernel,bias regularization\n",
    "'''\n",
    "#code starts\n",
    "    # Initial convolution\n",
    "    x = \n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter =\n",
    "        # add transition\n",
    "        x = \n",
    "\n",
    "    # The last denseblock does not have a transition\n",
    "    x, nb_filter = denseblock(x, concat_axis, nb_layers,\n",
    "                              nb_filter, growth_rate, \n",
    "                              dropout_rate=dropout_rate,\n",
    "                              weight_decay=weight_decay)\n",
    "\n",
    "    x =\n",
    "    x =\n",
    "    x =\n",
    "    x = \n",
    "#code ends\n",
    "    densenet = Model(inputs=[model_input], outputs=[x], name=\"DenseNet\")\n",
    "\n",
    "    return densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training:\n",
    "\n",
    "### then we define a function to train our model on cifar-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_cifar10(batch_size,\n",
    "                nb_epoch,\n",
    "                depth,\n",
    "                nb_dense_block,\n",
    "                nb_filter,\n",
    "                growth_rate,\n",
    "                dropout_rate,\n",
    "                learning_rate,\n",
    "                weight_decay,\n",
    "                plot_architecture):\n",
    "    \"\"\" Run CIFAR10 experiments\n",
    "    :param batch_size: int -- batch size\n",
    "    :param nb_epoch: int -- number of training epochs\n",
    "    :param depth: int -- network depth\n",
    "    :param nb_dense_block: int -- number of dense blocks\n",
    "    :param nb_filter: int -- initial number of conv filter\n",
    "    :param growth_rate: int -- number of new filters added by conv layers\n",
    "    :param dropout_rate: float -- dropout rate\n",
    "    :param learning_rate: float -- learning rate\n",
    "    :param weight_decay: float -- weight decay\n",
    "    :param plot_architecture: bool -- whether to plot network architecture\n",
    "    \"\"\"\n",
    "\n",
    "    ###################\n",
    "    # Data processing #\n",
    "    ###################\n",
    "\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    nb_classes = len(np.unique(y_train))\n",
    "    img_dim = X_train.shape[1:]\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        n_channels = X_train.shape[1]\n",
    "    else:\n",
    "        n_channels = X_train.shape[-1]\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "\n",
    "    # Normalisation\n",
    "    X = np.vstack((X_train, X_test))\n",
    "    # 2 cases depending on the image ordering\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        for i in range(n_channels):\n",
    "            mean = np.mean(X[:, i, :, :])\n",
    "            std = np.std(X[:, i, :, :])\n",
    "            X_train[:, i, :, :] = (X_train[:, i, :, :] - mean) / std\n",
    "            X_test[:, i, :, :] = (X_test[:, i, :, :] - mean) / std\n",
    "\n",
    "    elif K.image_data_format() == \"channels_last\":\n",
    "        for i in range(n_channels):\n",
    "            mean = np.mean(X[:, :, :, i])\n",
    "            std = np.std(X[:, :, :, i])\n",
    "            X_train[:, :, :, i] = (X_train[:, :, :, i] - mean) / std\n",
    "            X_test[:, :, :, i] = (X_test[:, :, :, i] - mean) / std\n",
    "\n",
    "    ###################\n",
    "    # Construct model #\n",
    "    ###################\n",
    "\n",
    "    model = densenet.DenseNet(nb_classes,\n",
    "                              img_dim,\n",
    "                              depth,\n",
    "                              nb_dense_block,\n",
    "                              growth_rate,\n",
    "                              nb_filter,\n",
    "                              dropout_rate=dropout_rate,\n",
    "                              weight_decay=weight_decay)\n",
    "    # Model output\n",
    "    model.summary()\n",
    "\n",
    "    # Build optimizer\n",
    "    opt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    if plot_architecture:\n",
    "        from keras.utils.visualize_util import plot\n",
    "        plot(model, to_file='./figures/densenet_archi.png', show_shapes=True)\n",
    "\n",
    "    ####################\n",
    "    # Network training #\n",
    "    ####################\n",
    "\n",
    "    print(\"Training\")\n",
    "\n",
    "    list_train_loss = []\n",
    "    list_test_loss = []\n",
    "    list_learning_rate = []\n",
    "\n",
    "    for e in range(nb_epoch):\n",
    "\n",
    "        if e == int(0.5 * nb_epoch):\n",
    "            K.set_value(model.optimizer.lr, np.float32(learning_rate / 10.))\n",
    "\n",
    "        if e == int(0.75 * nb_epoch):\n",
    "            K.set_value(model.optimizer.lr, np.float32(learning_rate / 100.))\n",
    "\n",
    "        split_size = batch_size\n",
    "        num_splits = X_train.shape[0] / split_size\n",
    "        arr_splits = np.array_split(np.arange(X_train.shape[0]), num_splits)\n",
    "\n",
    "        l_train_loss = []\n",
    "        start = time.time()\n",
    "\n",
    "        for batch_idx in arr_splits:\n",
    "\n",
    "            X_batch, Y_batch = X_train[batch_idx], Y_train[batch_idx]\n",
    "            train_logloss, train_acc = model.train_on_batch(X_batch, Y_batch)\n",
    "\n",
    "            l_train_loss.append([train_logloss, train_acc])\n",
    "\n",
    "        test_logloss, test_acc = model.evaluate(X_test,\n",
    "                                                Y_test,\n",
    "                                                verbose=0,\n",
    "                                                batch_size=64)\n",
    "        list_train_loss.append(np.mean(np.array(l_train_loss), 0).tolist())\n",
    "        list_test_loss.append([test_logloss, test_acc])\n",
    "        list_learning_rate.append(float(K.get_value(model.optimizer.lr)))\n",
    "        # to convert numpy array to json serializable\n",
    "        print('Epoch %s/%s, Time: %s' % (e + 1, nb_epoch, time.time() - start))\n",
    "\n",
    "        d_log = {}\n",
    "        d_log[\"batch_size\"] = batch_size\n",
    "        d_log[\"nb_epoch\"] = nb_epoch\n",
    "        d_log[\"optimizer\"] = opt.get_config()\n",
    "        d_log[\"train_loss\"] = list_train_loss\n",
    "        d_log[\"test_loss\"] = list_test_loss\n",
    "        d_log[\"learning_rate\"] = list_learning_rate\n",
    "\n",
    "        json_file = os.path.join('./log/experiment_log_cifar10.json')\n",
    "        with open(json_file, 'w') as fp:\n",
    "            json.dump(d_log, fp, indent=4, sort_keys=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameter configurationï¼š\n",
    "### last but not least,we should set some parameters such as batch size,growth rate to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9ee4e17b6685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 plot_architecture)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-6d982ce06e97>\u001b[0m in \u001b[0;36mrun_cifar10\u001b[0;34m(batch_size, nb_epoch, depth, nb_dense_block, nb_filter, growth_rate, dropout_rate, learning_rate, weight_decay, plot_architecture)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# the data, shuffled and split between train and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mnb_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/datasets/cifar10.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdirname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cifar-10-batches-py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muntar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mnum_train_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \"\"\"\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "'''you can set pass-in params here by modifying the following value:'''\n",
    "# set params:\n",
    "    batch_size=64\n",
    "    nb_epoch=30\n",
    "    depth=7\n",
    "    nb_dense_block=1\n",
    "    nb_filter=16\n",
    "    growth_rate=12\n",
    "    dropout_rate=0.2\n",
    "    learning_rate=1E-3\n",
    "    weight_decay=1E-4\n",
    "    plot_architecture=False\n",
    "    list_dir = [\"./log\", \"./figures\"]\n",
    "    for d in list_dir:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "\n",
    "    run_cifar10(batch_size,\n",
    "                nb_epoch,\n",
    "                depth,\n",
    "                nb_dense_block,\n",
    "                nb_filter,\n",
    "                growth_rate,\n",
    "                dropout_rate,\n",
    "                learning_rate,\n",
    "                weight_decay,\n",
    "                plot_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result analysis:\n",
    "### if you reach this step ,congratulations! you've successfully implemented your own DenseNet model,let's evaluate our model to have a look at this network's powerful performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './log/experiment_log_cifar10.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-393fd0e4c98f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mplot_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-393fd0e4c98f>\u001b[0m in \u001b[0;36mplot_cifar10\u001b[0;34m(save)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./log/experiment_log_cifar10.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './log/experiment_log_cifar10.json'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_cifar10(save=True):\n",
    "\n",
    "    with open(\"./log/experiment_log_cifar10.json\", \"r\") as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    train_accuracy = 100 * (np.array(d[\"train_loss\"])[:, 1])\n",
    "    test_accuracy = 100 * (np.array(d[\"test_loss\"])[:, 1])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.plot(train_accuracy, color=\"tomato\", linewidth=2, label='train_acc')\n",
    "    ax1.plot(test_accuracy, color=\"steelblue\", linewidth=2, label='test_acc')\n",
    "    ax1.legend(loc=0)\n",
    "\n",
    "    train_loss = np.array(d[\"train_loss\"])[:, 0]\n",
    "    test_loss = np.array(d[\"test_loss\"])[:, 0]\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.plot(train_loss, '--', color=\"tomato\", linewidth=2, label='train_loss')\n",
    "    ax2.plot(test_loss, '--', color=\"steelblue\", linewidth=2, label='test_loss')\n",
    "    ax2.legend(loc=1)\n",
    "\n",
    "    ax1.grid(True)\n",
    "\n",
    "    if save:\n",
    "        fig.savefig('./figures/plot_cifar10.svg')\n",
    "\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plot_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
